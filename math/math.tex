\part{Mathematics}
\section{Numbers}

\subsection{Triangle inequality}

\subsubsection{Number inequality}

We start with the obvious inequality that the absolute value of a number
$\left|a\right|$ is always greater or equal the number $a$ itself 
\begin{align}
|a| \geq a
\end{align} 
Using this we can prove that for numbers $a$ and $b$ we have 
\begin{align}
|a| + |b|  \geq |a+b|
\end{align} 
because 
\begin{align}
(|a| + |b|)^2  &\geq (|a+b|)^2 \\
|a|^2  + 2|a||b| + |b|^2 &\geq |a^2 + 2ab + b^2| \\
2|a||b| &\geq 2|ab|
\end{align} 
The last line is always true if $|a| \geq a$, which completes our
prove. 

\subsection{Cauchy-Schwarz inequality} 
Because
$\sin^2(\phi) + \cos^2(\phi) = 1$ we have 
\begin{align}
|a|^2|b|^2 = |a|^2|b|^2 (\sin^2(\phi) + \cos^2(\phi))
\end{align} 
With the definition of the scalar product
$|a\cdot b| = |a||b|\cos(\phi)$ and the cross product
$|a\times b| = |a||b|\sin(\phi)$ of vectors $a$ and $b$ we can
write this expression as 
\begin{align}
|a|^2|b|^2 = |a\times b|^2 + |a\cdot b|^2 
\label{eq:lagrident}
\end{align} 
This is called \href{https://en.wikipedia.org/wiki/Lagrange\%27s_identity}{Lagrange's identity}. Since all terms are squared and
therefor positive we immediatelly can derive the inequalities 
\begin{align}
|a|^2|b|^2 &\geq |a\cdot b|^2 \\
|a|^2|b|^2 &\geq |a\times b|^2
\end{align} 
The first of these equations is the Cauchy-Schwarz inequality. The
second equations doesn't seem to have a name in the literature.

The Cauchy-Schwarz inequality comes in many different forms. For example
for $n$-dimensional vectors it can be written in cartesian coordinates
like 
\begin{align}
\sum (a_i)^2 \sum (b_i)^2 \geq \left(\sum a_i b_i \right)^2
\end{align} 
This can even be generalized to uncountable infinite dimensional
vectors (also called continuous square integrable functions) like 
\begin{align}
\left(\int \left|a(x)\right|^2 dx\right) \cdot \left(\int \left|b(x)\right|^2 dx\right) \geq \left|\int a(x) \cdot b(x)dx\right|^2
\end{align}

\section{Matrices}

In the following we want to restrict our discussion to matrices with
real entries.

\subsection{Square matrices}
Every square matrix can be split into a symmetric and an antisymmetric
(skew-symmetric) part 
\begin{align}
A=\underbrace{\frac{1}{2}\lra{A + A^T}}_{\text{symmetric}}
+\underbrace{\frac{1}{2}\lra{A - A^T}}_{\text{antisymmetric}}
\end{align}


\subsection{Symmetric matrices}

A symmetric matrix is a square matrix, that is equal to it's transpose.
\begin{align}
A = A^T
\end{align} 
The sum of two symmetric matrices is again a symmetric matrix, but
the product of two matrices is in general not symmetric. The product of
two symmetric matrices $A$ and $B$ is symmetric only, if the two
matrices commute: 
\begin{align}
AB = (AB)^T \ \text{if}\ AB = BA
\end{align} 
Symmetric matrices with real entries have only real eigenvalues. That
is why in principle, every symmetric matrix is equivalent to a diagonal
matrix with its eigenvalues being the entries on the diagonal.

\subsection{Applications of Matrices}
\subsubsection{Solving unsolvable equations}

We start with a simple equation: 
\begin{align}
A^2 * 1 = 1 
\end{align} 
What is the transformation A, when applied twice, which turns 1 into
1 ? The answer is simple: $A=1$ or $A=-1$. But what is the
transformation $B$, when applied twice, which turns 1 into -1 ? 
\begin{align}
B^2 * 1 = -1 
\end{align} 
The answer it turns out, is not so simple. Based on the common rules
of multiplication for real numbers, there seems to be no way to solve
this equation for $B$.

However there is a way out of the dilemma. The reader might have
noticed, that we didn't call $A$ or $B$ a number or a variable, but
a transformation. We could have also called it also an operator. This
should be a hint, that maybe simple numbers are not enough to solve such
an equation.

So let's think a bit. Can this equation maybe interpreted in a
geometrical way? Let's imagine the line of numbers. What we want is a
way, to move or transform the point 1 to the point -1, but doing it with
two steps (One step would be simple, this could be reflection at the
zero point). If we draw the line on a sheet of paper and look from far
away the solution might be become more obvious: The line is embedded on
the two dimensional surface of the paper! So we might interpret our
source point 1 in fact as a vector (1,0) and our target point -1 as
vector (-1,0). Maybe this helps. Let's rewrite our equation in two
dimensions 
\begin{align}
B^2 * \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} -1 \\ 0 \end{pmatrix} 
\end{align} 
So what we want is a transformation $B$, which when used twice on
the vector (1,0) will turn the vector into the vector (-1,0). Vector
(-1,0) is pointing into the opposite direction of vector (1,0), so it is
basically rotated by 180 degrees. So we are searching for a
transformation which rotates a vector in two steps by 180 degrees. Now
it should be obvious that the solution for $B$ must be a
transformation, which rotates a vector by 90 degrees, in short
$B = R(90^{\circ})$ (or a rotation in the other direction by -90
degrees).

So we found a solution by geometrical intuition, let's try to make it
precise. From linear algebra we know, that a transformation which turns
a 2D vector into another 2D vector must be a 2x2 matrix. So let's
rewrite the equation as a matrix equation 
\begin{align}
\begin{pmatrix} a & b \\ c & d \end{pmatrix} * \begin{pmatrix} a & b \\ c & d \end{pmatrix} * \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} -1 \\ 0 \end{pmatrix} 
\end{align} 
If we compute the matrix product on the left hand side we get 
\begin{align}
\begin{pmatrix} a^2 + bc & b (a+d) \\ c(a+d) & d^2 + bc \end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} -1 \\ 0 \end{pmatrix} 
\end{align} 
Further simplifying we get two equations 
\begin{align}
a^2+bc = -1 &\ & ac + cd = 0
\end{align} 
To solve this equation one might attempt to set $c=0$. But in this
case we would end up where we started because the equation left would be
$a^2=-1$. So we have to assume $c \neq 0$ to find a sensible
solution. So we get 
\begin{align}
c = -\frac{a^2+1}{b} &\ & a = -d
\end{align} 
From these equation we see that we also have to assume $b \neq 0$.
But without loss of generality we can set $a=0$,$d=0$ and are left
with 
\begin{align}
bc = -1
\end{align} 
If we restrict ourself to integer numbers, we finally have two
solutions for our transformation 
\begin{align}
B = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}  &\ & B^* = \begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix}
\end{align} 
So what seemed impossible to solve in 1D with simple numbers turned
out to have quite simple solutions in 2D in the form of 2x2 matrices.

\subsubsection{Solving polynomial equations}
A polynomial equation of order $n$ is an equation that look like 
\begin{align}
a_nx^n + a_{n-1}x^{n-1} + \cdots + a_1 x + a_0 = 0
\end{align} 
Solving these kind of equations has been a hobby of mathematicans
since the invention of math. But whereas quadratic equations ($n=2$)
could be solved since ancient times, finding a general solution for
cubic ($n=3$) and quartic ($n=4$) equations turned out to be much
harder. Rafael Bombelli made a crucial step in 1572 when - in a
desparate move - he invented complex numbers to solve cubic equations,
which had been unsolvable up to that time. Had he known matrices and
linear algebra, the invention of ``complex numbers'' would have been
unnecessary.

This is so, because there is a deep connection between polynominal
equations and matrices. Actually it turns out that \textbf{any
polynomial with degree $n$ is the characteristic polynomial of some
\href{https://en.wikipedia.org/wiki/Companion_matrix}{companion matrix}
of order $n$}. So the problem of solving a polynominal equation is
equivalent to solving the characteristic equation of the companion
matrix. But solving the characteristic equation of a matrix means
computing the eigenvalues of that matrix. And computing eigenvalues has
a simple geometric meaning: They give the factor by which an eigenvector
(a vector which direction is left unchanged by the matrix
transformation) is stretched by the matrix transformation. The
eigenvalues are the scale factors of the linear transformation
represented by the matrix. This means that solving a polynomial equation
is equivalent to computing the scale factors of a corresponding linear
transformation.

Knowning this we can nowadays understand, why some polynominal equations
have no solutions. These are the equations, which correspond to
matrices, which don't have eigenvalues, meaning the transformation
doesn't leave any vector unchanged. From linear algebra we know, that
these transformations describe rotations. And this is the connection to
the 2d rotation matrix we found in the previous chapter. The polynominal
equation 
\begin{align}
x^2 = -1
\end{align} 
has no solution, because it corresponds to a matrix describing a 90
degree rotation.

On the other side the
\href{https://en.wikipedia.org/wiki/Fundamental_theorem_of_algebra}{fundamental
theorem of algebra} is easy to understand, when thinking of polynomials
as being represented by matrices. The number of solutions to a
polynomial of degree $n$ is the same as the number of eigenvalues of
the corresponding companion matrix.

\section{Complex Numbers}
Complex numbers are not numbers. They cannot be ordered according to their size. This basic insight makes clear that trying to work with complex numbers like with usual “real numbers” must fail (e.g. division doesn’t work) and in general is also the reason for the big confusion around them.

But complex numbers are also no vectors (in the geometrical sense). The multiplication rule for complex numbers is completely different from the usual scalar product of geometrical vectors. Multiplying two complex numbers yields another complex number, whereas the usual scalar multiplication of geometrical vectors yields a scalar. So although a complex number can be represented as a set of two numbers, this set of two numbers should not(!) be visualized as geometrical vector.

Instead the modern view is that complex numbers are 2D matrices. They represent the group of (antisymmetric) 2D rotation matrices (\url{https://en.wikipedia.org/wiki/Complex_number#Matrix_representation_of_complex_numbers}). All the features of complex numbers follow naturally from this representation, e.g. multiplying two matrices yields another matrix. Unfortunately most textbooks don’t even mention the matrix representation of complex numbers, although this really makes clear what complex “numbers” are, how they can be extended (e.g. quaternions are 3D rotation matrices) and how they fit into the bigger picture which is \url{https://en.wikipedia.org/wiki/Group_theory}.

\section{Tensor calculus}

\subsection{Levi-Civita-Symbol}
The symbol $\epsilon_{ijkl\ldots}$ is called Levi-Civita symbol and defined as
follows:
\begin{align}
\epsilon_{ijkl\ldots}=
\begin{cases} 	 1 & \text{if $i,j,k,l\ldots$ is an even permutation}, \\
					-1 & \text{if $i,j,k,l\ldots$ is an odd permutation}, \\
					 0 & \text{otherwise (two or more labels are the same)}.
\end{cases}				  
\end{align}
Therefore the Levi-Civita-Symbol will change its sign, if two labels are
exchanged
\begin{align*}
\epsilon_{ijkl\ldots u \ldots v} = -\epsilon_{ijkl\ldots v \ldots u}.
\end{align*}
The Levi-Civita-Symbol is not a tensor, but a pseudotensor, because it
transforms like a tensor under rotation, but not under reflection
\citep{Pope2000}.\footnote{Is it possible to fix up the Levi-Civita-Symbol so
it becomes a real tensor?}

\subsubsection{Levi-Civita-Symbol in 3D}
In 3D only 6 of the 27 components of the Levi-Civita-Symbolare are unequal zero
\begin{align*}
\epsilon_{123}=\epsilon_{312}=\epsilon_{231}&=1 \\
\epsilon_{321}=\epsilon_{132}=\epsilon_{213}&=-1
\end{align*}
The Levi-Civita-Symbol in 3D is most often used to express components
of a cross product of vectors in cartesian tensor notation
\begin{align*}
\lrb{\vec{u} \times \vec{v}}_i = \epsilon_{ijk} u_j v_k
=&\ \epsilon_{i11} u_1 v_1 + \epsilon_{i12} u_1 v_2 +\epsilon_{i13} u_1 v_3\\
&+\epsilon_{i21} u_2 v_1 + \epsilon_{i22} u_2 v_2 + \epsilon_{i23} u_2 v_3\\
&+\epsilon_{i31} u_3 v_1 + \epsilon_{i32} u_3 v_2 + \epsilon_{i32} u_3 v_3\\
=&\ \delta_{i3} u_1 v_2 -\delta_{i2} u_1 v_3 -\delta_{i3} u_2 v_1\\
&+\delta_{i1} u_2 v_3 + \delta_{i2} u_3 v_1 - \delta_{i1} u_3 v_2\\
=&\ \delta_{i1}\lra{u_2 v_3-u_3 v_2}\\
&+\delta_{i2}\lra{u_3 v_1-u_1 v_3}\\
&+\delta_{i3}\lra{u_1 v_2-u_2 v_1}
\end{align*}
or the components of the curl of a vector field
\begin{align*}
\lrb{\nabla \times \vec{v}}_i = \epsilon_{ijk} \pd{r_j} v_k
\end{align*}
To express double cross product other more complicated expressions we need
the following important relation between the Kronecker Delta and the
Levi-Civita-Symbol 
\begin{align}
\begin{split}
\epsilon_{ijk}\epsilon_{lmn}=&
\begin{vmatrix}
  \delta_{il} & \delta_{im} & \delta_{in} \\
  \delta_{jl} & \delta_{jm} & \delta_{jn} \\
  \delta_{kl} & \delta_{km} & \delta_{kn}
\end{vmatrix}
\\
=&\ \delta_{il}\delta_{jm}\delta_{kn}
+\delta_{im}\delta_{jn}\delta_{kl}
+\delta_{in}\delta_{jl}\delta_{km}\\
&-\delta_{in}\delta_{jm}\delta_{kl}
-\delta_{il}\delta_{jn}\delta_{km}
-\delta_{im}\delta_{jl}\delta_{kn}
\end{split}
\end{align}
From this relation we can derive the following
\begin{align}
\begin{split}
\epsilon_{ijk}\epsilon_{imn}
=&\ \delta_{ii}\delta_{jm}\delta_{kn}
+\delta_{im}\delta_{jn}\delta_{ki}
+\delta_{in}\delta_{ji}\delta_{km}\\
&-\delta_{in}\delta_{jm}\delta_{ki}
-\delta_{ii}\delta_{jn}\delta_{km}
-\delta_{im}\delta_{ji}\delta_{kn}\\
=&\ 3\delta_{jm}\delta_{kn}
+\delta_{km}\delta_{jn}
+\delta_{jn}\delta_{km}\\
&-\delta_{kn}\delta_{jm}
-3\delta_{jn}\delta_{km}
-\delta_{jm}\delta_{kn}\\
=&\ \delta_{jm}\delta_{kn}-\delta_{jn}\delta_{km}
\end{split}
\\
\begin{split}
\epsilon_{ijk}\epsilon_{ijn}
=&\ \delta_{jj}\delta_{kn}-\delta_{jn}\delta_{kj}\\
=&\ 3\delta_{kn}-\delta_{kn}\\
=&\ 2\delta_{kn}
\end{split}
\\
\begin{split}
\epsilon_{ijk}\epsilon_{ijk}
=&\ 2\delta_{kk}\\
=&\ 6
\end{split}
\end{align}

\subsection{Properties of second order tensors}
A second order tensor can be decomposed into a symmetric and an antisymmetric
part in the following way\footnote{How does this look like in general
curvilinear coordinates?}
\begin{align}
T_{ij}=\underbrace{\frac{1}{2}\lra{T_{ij}+T_{ji}}}_{\text{symmetric}}
+\underbrace{\frac{1}{2}\lra{T_{ij}-T_{ji}}}_{\text{antisymmetric}}
\end{align}
It can also be decomposed into an isotropic and deviatoric part by subtracting
and adding the trace of the tensor like
\begin{align}
T_{ij}=\underbrace{\frac{1}{n}\delta_{ij} T_{kk}}_{\text{isotropic}}
+\underbrace{T_{ij}-\frac{1}{n}\delta_{ij} T_{kk}}_{\text{deviatoric, 
tracefree}}
\end{align}
Combining these two relations yields the general decomposition
\begin{align}
\begin{split}
T_{ij}=&
\overbrace{\frac{1}{n}\delta_{ij} T_{kk}}^{\text{isotropic}}
+
\overbrace{
\underbrace{\frac{1}{2}\lra{T_{ij}+T_{ji}-\frac{2}{n}\delta_{ij}
T_{kk}}}_{\text{symmetric, tracefree}}
+\frac{1}{2}\lra{T_{ij}-T_{ji}}}^{\text{deviatoric, tracefree}}\\[-1em]
&\underbrace{\hphantom{\frac{1}{n}\delta_{ij}
T_{kk}+\frac{1}{2}\lra{T_{ij}+T_{ji}-\frac{2}{n}\delta_{ij}
T_{kk}}}}_{\text{symmetric}}
\hphantom{+}
\underbrace{\hphantom{\frac{1}{2}\lra{T_{ij}-T_{ji}}}}_{\text{
antisymmetric}}
\end{split}
\end{align}

An interesting relation can be found when computing the contraction of a
unsymmetric tensor $U_{ij} \neq U_{ji}$ with a  symmetric tensor
$V_{ij}=V_{ji}$
\begin{align}
U_{ij}V_{ij}
= \frac{1}{2} U_{ij}V_{ij} + \frac{1}{2} U_{ji}V_{ji} 
= \frac{1}{2} U_{ij}V_{ij} + \frac{1}{2} U_{ji}V_{ij}
= \frac{1}{2} \lra{U_{ij}+U_{ji}} V_{ij}
\label{eq:uscontr}
\end{align}
In analogy one finds for the contraction of an unsymmetric tensor $U_{ij}$ with an
antisymmetric tensor $W_{ij}=-W_{ji}$
\begin{align}
U_{ij}W_{ij} = \frac{1}{2} \lra{U_{ij}-U_{ji}} W_{ij}
\label{eq:uascontr}
\end{align}

\section{Vector calculus}

\subsection{Longitudinal and transversal projection of vectors}

%\begin{figure}[htp]
%\centering
%\resizebox{0.4\textwidth}{!}{
%\input{projection.pstex_t}}
%\caption{Projection of a vector $\vec{a}$ on a vector $\vec{b}$.}
%\label{fig:projection}
%\end{figure}

A vector $\vec{a}$ can be split into two parts: the longitudinal part $\vec{a}_{\parallel}$, 
which is parallel to another vector $\vec{b}$ and the transversal part $\vec{a}_{\perp}$, 
which is perpendicular to $\vec{b}$. The length of the longitudinal part $a_{\parallel}$ 
and the transversal part $a_{\perp}$ can be computed from geometry (see figure \ref{fig:projection})
\begin{align}
\frac{a_{\parallel}}{a}=\cos \alpha & \Rightarrow
a_{\parallel}= a \cos\alpha = \frac{a b \cos \alpha}{b} = \frac{\vec{a}\cdot\vec{b}}{b},\\
\frac{a_{\perp}}{a}=\sin \alpha & \Rightarrow 
a_{\perp}= a \sin\alpha = \frac{a b \sin \alpha}{b} = \frac{\abs{\vec{a}\times\vec{b}}}{b}. \label{eq:protrans}
\end{align}
But from the Pythagorean theorem we can get another expression for the length of the transversal
part
\begin{align}
a^2=a_{\parallel}^2+a_{\perp}^2 & \Rightarrow
a_{\perp}^2 = a^2-a_{\parallel}^2= a^2-\frac{(\vec{a}\cdot\vec{b})^2}{b^2}.\label{eq:protrans2}
\end{align}
Substituting equation \eqref{eq:protrans} in equation \eqref{eq:protrans2} we get
\begin{align*}
\frac{(\abs{\vec{a}\times\vec{b}})^2}{b^2}=a^2-\frac{(\vec{a}\cdot\vec{b})^2}{b^2},
\end{align*}
which leads us to the following expression for the square of the norm of the cross product
\begin{align}
\abs{\vec{a}\times\vec{b}}^2 = (a b)^2 - (\vec{a}\cdot\vec{b})^2.
\end{align}
This is again Lagrange's identity (see \eqref{eq:lagrident}). 

\subsection{Vector identities}
In this chapter we show the derivation of some vector quantities in cartesian
tensor notation.
\subsubsection{$(\vec{u}\cdot\nabla) \vec{v}$} \label{vecid01}
For some arbitraty vectors $u_i, v_i$ we can write
\begin{align*}
u_j \pd{r_j} v_i &= \overbrace{u_j \pd{r_i} v_j - u_j \pd{r_i} v_j}^0 
+ u_j\pd{r_j} v_i \\
&= u_j \pd{r_i} v_j - \delta_{ik}\delta_{jl} u_j \pd{r_k} v_l 
+\delta_{il}\delta_{jk} u_j \pd{r_k} v_l \\
&= u_j \pd{r_i} v_j 
- (\delta_{ik}\delta_{jl}-\delta_{il}\delta_{jk})u_j \pd{r_k} v_l \\
&= u_j \pd{r_i} v_j - \epsilon_{mij} \epsilon_{mkl} u_j \pd{r_k} v_l \\
&= u_j \pd{r_i} v_j - \epsilon_{ijm} u_j \epsilon_{mkl} \pd{r_k} v_l.
\end{align*}
In vector notation this can be expressed like
\begin{align}
(\vec{u}\cdot\nabla) \vec{v} = 
\vec{u}\cdot (\nabla \vec{v})-\vec{u} \times (\nabla \times \vec{v})
\label{eq:vecid01}
\end{align}
\subsubsection{$(\vec{v}\cdot\nabla) \vec{v}$}
Inserting $u_j=v_j$ into equation \eqref{eq:vecid01} yields
\begin{align*}
v_j \pd{r_j} v_i = v_j \pd{r_i} v_j 
- \epsilon_{ijm} v_j \epsilon_{mkl} \pd{r_k} v_l
\end{align*}
For $v_j \pd{r_i} v_j$ we can write
\begin{align*}
v_j \pd{r_i} v_j &= \pd{r_i} (v_j v_j) - v_j \pd{r_i} v_j
\end{align*}
and therefore
\begin{align*}
v_j \pd{r_i} v_j = \pd{r_i} \lra{\frac{1}{2} v_j v_j}.
\end{align*}
Using this we get for
\begin{align*}
v_j \pd{r_j} v_i = \pd{r_i} \lra{\frac{1}{2} v_j v_j}
- \epsilon_{ijm} v_j \epsilon_{mkl} \pd{r_k} v_l
\end{align*}
or in vector notation
\begin{align}
(\vec{v}\cdot\nabla) \vec{v} = 
\frac{1}{2} \nabla \vec{v}^2-\vec{v} \times (\nabla \times \vec{v})
\label{eq:vecid02}
\end{align}
\subsubsection{$\nabla \times \lra{\vec{u} \times \vec{v}}$}
The $i$-th component of the rotation of a cross product of two vectors is
\begin{align*}
\lrb{\nabla \times \lra{\vec{u} \times \vec{v}}}_i 
&= \epsilon_{ijk} \pd{r_j} \epsilon_{klm} u_l v_m \\
&= \epsilon_{kij} \epsilon_{klm} \pd{r_j} (u_l v_m) \\
&= (\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}) \pd{r_j} (u_l v_m) \\
&= \pd{r_j} (u_i v_j) - \pd{r_j} (u_j v_i) \\
&= u_i \pd{r_j} v_j + v_j \pd{r_j} u_i - u_j \pd{r_j} v_i - v_i \pd{r_j} u_j
\end{align*}
It can be written in vector notation like
\begin{align}
\nabla \times \lra{\vec{u} \times \vec{v}} 
= \vec{u} (\nabla \cdot \vec{v}) - \vec{v} (\nabla \cdot \vec{u})
+ (\vec{v} \cdot \nabla) \vec{u} - (\vec{u} \cdot \nabla) \vec{v}
\end{align}

\subsection{Jacobian determinant}
\subsubsection{Definition}\label{jacobi}
If the cartesian coordinates are given as functions of general curvilinear
coordinates $a_1, a_2, a_3$, i.e.
\begin{align}
 x=x(a_1, a_2, a_3),&&  y=y(a_1, a_2, a_3),&& z=z(a_1, a_2, a_3),
\end{align}
it follows for the infinitesimal volume element in curvilinear coordinates
\begin{align}
dV=\abs{J} da_1 da_2 da_3.
\end{align}
Thereby the Jacobian determinant $J$ is defined as
\begin{align}
J = \abs{\frac{\partial(x,y,z)}{\partial (a_1, a_2,a_3)}}=
\begin{vmatrix} \frac{\partial x}{\partial a_1} & 
					 \frac{\partial x}{\partial a_2} &
					 \frac{\partial x}{\partial a_3} \\
					 \frac{\partial y}{\partial a_1} & 
					 \frac{\partial y}{\partial a_2} &
					 \frac{\partial y}{\partial a_3} \\
					 \frac{\partial z}{\partial a_1} & 
					 \frac{\partial z}{\partial a_2} &
					 \frac{\partial z}{\partial a_3} 
\end{vmatrix}=
\sum_{i,j,k} \epsilon_{ijk} \frac{\partial x}{\partial a_i} 
					 				 \frac{\partial y}{\partial a_j} 
					 				 \frac{\partial z}{\partial a_k}.
\end{align}

\subsubsection{Time derivative of Jacobian determinant}\label{jacdt}
With $v_i= \frac{dx_i}{dt}$ it follows for the time derivative of $J$
\begin{align}
\frac{dJ}{dt}&=\frac{d}{dt} \sum_{i,j,k} \epsilon_{ijk} 
\frac{\partial x}{\partial a_i} \frac{\partial y}{\partial a_j} 
\frac{\partial z}{\partial a_k} \\
&=\sum_{i,j,k} \epsilon_{ijk} \left(
\frac{\partial v_x}{\partial a_i} 
\frac{\partial y}{\partial a_j} 
\frac{\partial z}{\partial a_k}+
\frac{\partial x}{\partial a_i} 
\frac{\partial v_y}{\partial a_j} 
\frac{\partial z}{\partial a_k}+
\frac{\partial x}{\partial a_i} 
\frac{\partial y}{\partial a_j} 
\frac{\partial v_z}{\partial a_k} \right).
\end{align}
From $\frac{\partial v_k}{\partial a_i}=\sum_l \frac{\partial v_k}{\partial x_l}
\frac{\partial x_l}{\partial a_i}$ $(k \in x,y,z)$ we get:
\begin{align}
\frac{dJ}{dt}=\sum_{i,j,k,l} \epsilon_{ijk}
\left(
\frac{\partial v_x}{\partial x_l}
\frac{\partial x_l}{\partial a_i} 
\frac{\partial y}{\partial a_j} 
\frac{\partial z}{\partial a_k}+
\frac{\partial x}{\partial a_i}
\frac{\partial v_y}{\partial x_l}
\frac{\partial x_l}{\partial a_j} 
\frac{\partial z}{\partial a_k}+
\frac{\partial x}{\partial a_i} 
\frac{\partial y}{\partial a_j} 
\frac{\partial v_z}{\partial x_l}
\frac{\partial x_l}{\partial a_k} \right). 
\end{align}
Analysis of the first term in the bracket for
\begin{flalign*}
l&=1:
\frac{\partial v_x}{\partial x}
\frac{\partial x}{\partial a_i} 
\frac{\partial y}{\partial a_j} 
\frac{\partial z}{\partial a_k}, \\
l&=2:
\frac{\partial v_x}{\partial y}
\underbrace{\frac{\partial y}{\partial a_i} 
\frac{\partial y}{\partial a_j}}_{\text{symmetric}} 
\frac{\partial z}{\partial a_k}
\text{ is (multiplied with $\epsilon_{ijk}$) 0 when summed up},\\
l&=3:
\frac{\partial v_x}{\partial z}
\overbrace{\frac{\partial z}{\partial a_i}} 
\frac{\partial y}{\partial a_j} 
\overbrace{\frac{\partial z}{\partial a_k}}
\text{ is (multiplied with $\epsilon_{ijk}$) 0 when summed up}.
\end{flalign*}
So for symmetry reasons most of the contributions are zero when summed up. In we
end we see: For the first term there is only a non-vanishing contribution for
$l=1$, for the second term there is only one for $l=2$ and for the third term
there is only one for $l=3$. So we get
\begin{align}
\frac{dJ}{dt}=\sum_{i,j,k} \epsilon_{ijk} 
\left(\frac{\partial v_x}{\partial x} + \frac{\partial v_y}{\partial y} +
\frac{\partial v_z}{\partial z}\right) 
\frac{\partial x}{\partial a_i} 
\frac{\partial y}{\partial a_j} 
\frac{\partial z}{\partial a_k} = 
(\nabla \cdot \vec{v}) J.
\end{align}

\section{Fourier transform}
The continous one dimensional Fourier transform in $k$-space $F(k)$ of some
function in
$x$-space $f(x)$is defined like
\begin{align}
F(k)&=\ft\iinf f(x) e^{-ikx} dx\ \ \text{(Fourier transform)}
\end{align}
Using the Fourier transform on a function twice will produce the 
the original function again, but mirrored at the origin.
That's why one conventionally defines an inverse Fourier transform
\footnote{Nature does not know about the inverse Fourier transform.
If you have some optical device, which produces the Fourier transform
of some image and you use it twice on your image you will get a mirrored image!},
that will generate the not mirrored original function again, when used on the
Fourier transform of a function
\begin{align}
f(x)&=\ft\iinf F(k) e^{ikx} dx\ \ \text{(inverse Fourier transform)}
\end{align}
In three dimensions one defines the Fourier transform like
\begin{align}
F(\vec{k})=\ffft\iiiinf f(\vec{x}) e^{-i\vec{k}\vec{x}} dV \\
f(\vec{x})=\ffft\iiiinf F(\vec{k}) e^{i\vec{k}\vec{x}} dK
\end{align}
In cartesian coordinates the kernel of the Fourier transform 
$e^{-i\vec{k}\vec{x}}=e^{-i(k_xx+k_yy+k_zz)}$
separates and so the three dimensional Fourier transform of a function
which separates in cartesian coordinates $f(\vec{x})=a(x)b(y)c(z)$ is 
also separable
\begin{align*}
F(\vec{k})=A(k_x)B(k_y)C(k_z)=
\ffft\iinf a(x) e^{-ik_xx} dx 
\iinf b(y) e^{-ik_yy} dy
\iinf c(z) e^{-ik_zz} dz
\end{align*}
Thats why we like to use cartesian coordinates when we are using Fourier
transforms.
\subsection{Fourier transform of a delta function}
An important result can be derived by computing the inverse Fourier
transform of the Fourier transform of a delta function
\begin{align*}
\delta(x-x_0) &= \ft\iinf\ft\iinf \delta(x-x_0) e^{-ikx} dx e^{ikx} dk\\
&=\fft\iinf e^{-ikx_0} e^{ikx} dk = \fft\iinf e^{ik(x-x_0)} dk.
\end{align*}
From this we get, that the inverse Fourier transform of a
constant is the delta function
\begin{align*}
\ft\iinf e^{ik(x-x_0)} dk = \sqrt{2\pi} \delta(x-x_0).
\end{align*}
Taking the complex conjugate of this equation and making use of the fact
that $\delta^*(x-x_0) = \delta(x-x_0)$ we get as definition for the delta
function
\begin{align}
\delta(x-x_0)= \fft\iinf e^{\pm ik(x-x_0)} dk
\end{align}
Using this we can derive the astonishing result
\begin{align}
\iinf f(x) dx = \sqrt{2\pi} F(0)
\end{align}
as can be seen from
\begin{align*}
\iinf f(x) dx &= \iinf \ft \iinf F(k) e^{ikx} dk dx 
= \ft \iinf F(k) \underbrace{\iinf e^{ikx}}_{2\pi \delta(k)} dx dk\\
&= \sqrt{2\pi} \iinf F(k) \delta(k) dk = \sqrt{2\pi} F(0).
\end{align*}

\subsection{Convolution theorem}
The Fourier transform of the product of two function in $k$-space is
\begin{align*}
&\ft\iinf F(k) G(k) e^{ikx} dk =\\
&= \ft \iinf \ft \iinf f(x') e^{-ikx'} dx' \ft\iinf g(x'') e^{-ikx''} dx''
e^{ikx} dk\\
&=\ffft \iiiinf f(x') e^{-ikx'} g(x'') e^{-ikx''} e^{ikx} dx' dx'' dk\\
&=\ffft \iiinf f(x') g(x'') 
\underbrace{\iinf e^{-ik(x'+x''-x)} dk}_{2\pi \delta(x''-(x-x'))} dx' dx'' \\
&=\ft\iinf f(x') \iinf g(x'') \delta(x''-(x-x')) dx'' dx' \\
&=\ft \iinf f(x') g(x-x') dx' = h(x).
\end{align*}
The integral $h(x)$ is called convolution of the functions $f(x)$ and $g(x)$.
So the convolution theorem says that
\begin{align}
h(x) = \ft \iinf f(x') g(x-x') dx' = \ft\iinf F(k) G(k) e^{ikx} dk.
\end{align}

\subsection{Autocorrelation and Wiener-Khinchin Theorem}
The autocorrelation of a function is defined as\footnote{Note that
with our definition of the fourier transform we cannot define  
the autocorrelation function as $h_{AC}(x) = \ft \iinf f(x') f^*(x+x') dx'$
because we could then not derive the Wiener-Khinchin theorem.}
\begin{align}
h_{AC}(x) = \ft \iinf f^*(x') f(x+x') dx'
\end{align}
The Wiener-Khinchin Theorem states, that 
\begin{align}
\ft \iinf f^*(x') f(x+x') dx' = \ft \iinf \abs{F(k)}^2 e^{ikx} dk
\end{align}
which can be proved in analogy to the convolution theorem
\begin{align*}
&\ft \iinf f^*(x') f(x+x') dx' =\\
&=\ft \iinf f^*(x') \iinf f(x'') \delta(x''-(x+x')) dx'' dx'\\
&=\ft \iinf f^*(x') \iinf f(x'') \fft \iinf e^{-ik(x'+x''-x)} dk dx'' dx'\\
&=\ft \iinf \ft \iinf f^*(x') e^{ikx'} dx' \ft \iinf f(x'') e^{-ikx''} dx''
e^{ikx} dk\\
&=\ft \iinf F^*(k) F(k) e^{ikx} dk = \ft \iinf \abs{F(k)}^2 e^{ikx} dk.
\end{align*}
A special case of the Wiener-Khichnin theorem is Parseval's theorem
\begin{align}
\iinf \abs{f(x)}^2 dx = \iinf \abs{F(k)}^2 dk, 
\end{align}
which can be obtained from the Wiener-Khichnin theorem for $x=0$
\begin{align*}
h_{AC}(0)&= \ft \iinf f^*(x') f(x') dx' = \ft \iinf \abs{f(x)}^2 dx \\
&= \ft \iinf \abs{F(k)}^2 e^{ik0} dk = \ft \iinf \abs{F(k)}^2 dk
\end{align*}

\section{Basic probability theory}

\subsection{Definition of probability}

An experiment can measure if an event $A$ happend or not. If we repeat
an experiment $n$ times and we measure that the event $A$ happened
$m$ times we define the probability that the event $A$ happens as
\begin{align}
P(A) = \lim\limits_{n \to \infty} \frac{m}{n}
\end{align}

If event $C$ means that event $A$ \emph{or} event $B$ can happen
we write $C = A \cup B$. If event $D$ means that event $A$
\emph{and} event $B$ happen, we write $D = A \cap B$. Classical
probability theory is then based on the following three axioms (called
the Kolmogorov axioms):

\begin{enumerate}
\item
  Every event $A$ has a real non-negative probability $P(A) \ge 0$.
\item
  The probability that any event from the event space will happen is
  one: 
  
  $P(A \cup A^c) = 1$ (where $A^c$ is the complement event to
  $A$ in the event space)
\item
  The probabilities of mutually exclusive events ( $P(A \cap B) = 0$ )
  add: 
  
  $P(A \cup B) = P(A) + P(B)$
\end{enumerate}

From the last axiom it also follows that in general
\begin{align}
P(A \cup B) = P(A) + P(B) - P(A \cap B)
\end{align}
Although these axioms seem unavoidable it should be mentioned that
quantum probability theory violates axiom 1 and axiom 3. Axiom 3 is
violated, because measurement of events in quantum mechanics (QM) are
not commutative, meaning the measurement of event A often must influence
the measurement of event B. Axiom 1 is violated since QM must describe
interference effects between events and does this by introducing
\href{https://en.wikipedia.org/wiki/Negative_probability}{negative
probabilities} (To be more precise, the probability wave function of QM
is complex, because in the theory one must basically take the square
root of the negative probabilities). But as Dirac put it: ``Negative
probabilities should not be considered nonsense. They are well defined
concepts mathematically, like a negative sum of money. Negative
probabilities should be considered simply as things which do not appear
in experimental results.''

So it is possible to work with things like negative or complex
probabilities. But to be able to derive the central limit theorem it is
necessary that the three axioms of Kolmogorov for classical probability
hold.

\subsection{Random variables}
Examples for random variables are e.g.~the number on a thrown dice, the
lifetime of a instable radioactive nucleus, the amplitude of
athmospheric noise recorded by normal radio. In general a random
variable $X$ takes finite real values $x$ where the probability that
$X$ takes the value $x$ in a given intervall from $a$ to $b$
depends on the event described by $X$. We write 
\begin{align}
P(a < X < b) = \int_a^b f_X(x) dx 
\end{align} 
where $f_X(x)$ is the so called probability density function
characteristic for the event. We have to be careful to distinguish the
random variable $X$ from its value $x$ it takes after the
measurement. If $a < x < b$ then the probability $P(a < x < b)$ is
always one, because $x$ is just a number between $a$ and $b$. But
the value of $P(a < X < b)$ depends on the form of the probability
density function (Note however that $P(-\infty < x < \infty)$ =
$P(-\infty < X < \infty)$ according to the second axiom). So a random
variable is - despite its name - actually not a number or value, it is
\href{https://www.mathsisfun.com/data/random-variables.html}{a set of
possible values from a random experiment}, where each value has a
probability associated with it. To describe a experiment of throwing a
dice one could write 
\begin{align}
X = \{(1,1/6),(2,1/6),(3,1/6),(4,1/6),(5,1/6),(6,1/6)\}
\end{align} 
where the first value of each tuple is the possible outcome $x_i$
(called \href{https://en.wikipedia.org/wiki/Random_variate}{Random
Variate}), the second is the corresponding probability $p_i$. The
probability is 
\begin{align}
P(a < X < b) = \sum_{a < x_i < b} p_i  
\end{align}

or in case that the possible outcomes are a continous set $x(t)$ with
the corresponding
\href{https://en.wikipedia.org/wiki/Probability_density_function}{probability
density function} $p(t)$ 
\begin{align}
P(a < X < b) = \int_a^b p(t) dt  
\end{align}

In the special case $a=-\infty$ the probability $P$ only depends on
the upper limit $b$ 
\begin{align}
P(-\infty < X < b) = P(X < b) = \int_{-\infty}^b p(t) dt = F(b)  
\end{align} 
and we call $F(b)$ the
\href{https://en.wikipedia.org/wiki/Cumulative_distribution_function}{cumulative
distribution function}.

It should be noted that a possible outcome or random variate $x_i$ is
in principle a functional $x[p(t)]$ over the space of probability
density functions. This can be seen in the way how pseudo random numbers
are generated on a computer. One usually has a
\href{https://en.wikipedia.org/wiki/Pseudorandom_number_generator}{random
number generator} (often also called deterministic random number
generator, because starting from the same seed, it will generate the
same series of random numbers) generating uniformly distributed numbers
between 0 and 1. These uniformly distributed numbers are then
transformed to non-uniform random numbers by methods like
\href{https://en.wikipedia.org/wiki/Inverse_transform_sampling}{inverse
transform sampling} or
\href{https://en.wikipedia.org/wiki/Rejection_sampling}{rejection
sampling}, which basically make use of the probability density function
$p(t)$. So the scalar real value taken by a random variable generated
by a computer depends on the form of a function, making $x[p(t)]$ a
functional and the random variable $X$ a set of functionals.

\subsection{Multiple random variables}

Assume you throw one red and one blue dice in an experiment. The number
on the red dice would be random variable $X$, the number of the blue
dice random variable $Y$. Under normal circumstances the number on
each dice would be independent of each other. We say, the random
variables $X$ and $Y$ are uncorrelated. However, assume that the
dice are magnetic. In that case the number shown on each dice might not
be independent anymore.

To describe such an experiment where we measure a pair of random
variables we use a joint probability and a joint distribution function:
\begin{align}
P(a < X < b, c < Y < d) = \int_a^b\int_c^d f_{XY}(x,y) dx dy 
\end{align} 
In analogy we can also describe experiments with $n$ random
variables by using a $n$-dimensional joint distribution function (In
the limit that $n$ would approach an uncountable infinity the
probability would be expressed by an infinite dimensional integral, see
also
\href{https://en.wikipedia.org/wiki/Functional_integration}{Functional
integration})

Quantum mechanics = asymmetric covariance matrix??

\section{More topics}
\subsection{Linear algebra}

\begin{itemize}
\item \url{https://physics.stackexchange.com/questions/35562/is-a-1d-vector-also-a-scalar}
\item \url{https://math.stackexchange.com/questions/219434/is-a-one-by-one-matrix-just-a-number-scalar}
\item \url{https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Eigenvalues_of_geometric_transformations}
\end{itemize}
\subsubsection{Functions of matrices}
\begin{itemize}
\item \url{https://en.wikipedia.org/wiki/Matrix_function}
\item \url{https://en.wikipedia.org/wiki/Matrix_exponential}
\item \url{https://en.wikipedia.org/wiki/Logarithm_of_a_matrix}
\item \url{https://math.stackexchange.com/questions/1149598/how-to-solve-a-non-linear-matrix-equation-over-integer-numbers}
\end{itemize}
\subsection{Transformations and groups}

\begin{itemize}
\item Prove of eulers formula as a solution to 2d wave equation: \url{http://math.stackexchange.com/a/3512/27609}
\item \url{https://en.wikipedia.org/wiki/Linear_canonical_transformation}
\item \url{https://en.wikipedia.org/wiki/Hartley_transform}
\item \url{https://en.wikipedia.org/wiki/Split-complex_number}
\item \url{https://en.wikipedia.org/wiki/Dual_number} (\url{https://math.stackexchange.com/questions/1120720/are-dual-numbers-a-special-case-of-grassmann-number})
\item \url{https://en.wikipedia.org/wiki/Grassmann_number} \href{https://math.stackexchange.com/questions/1108045/relationship-between-levi-civita-symbol-and-grassmann-numbers}{Grassmann vectors}
\item \url{https://en.wikipedia.org/wiki/Quaternion} (\url{https://math.stackexchange.com/questions/147166/does-my-definition-of-double-complex-noncommutative-numbers-make-any-sense})
\item \url{https://math.stackexchange.com/questions/2083950/relationship-between-levi-civita-symbol-and-complex-quaternionic-numbers}
\end{itemize}
\subsection{Series}

\begin{itemize}
\item \url{http://blog.wolfram.com/2014/08/06/the-abcd-of-divergent-series}
\begin{itemize}
\item \url{http://physicsbuzz.physicscentral.com/2014/01/redux-does-1234-112-absolutely-not.html}
\item \url{https://www.quora.com/Whats-the-intuition-behind-the-equation-1+2+3+-cdots-tfrac-1-12}
\end{itemize}
\end{itemize}

\subsection{Calculus}

\subsubsection{Euler-MacLaurin}
\begin{itemize}
\item \url{https://people.csail.mit.edu/kuat/courses/euler-maclaurin.pdf}
\item \url{http://www.hep.caltech.edu/~phys199/lectures/lect5_6_ems.pdf}
\item \url{https://terrytao.wordpress.com/2010/04/10/the-euler-maclaurin-formula-bernoulli-numbers-the-zeta-function-and-real-variable-analytic-continuation}
\end{itemize}

\subsubsection{Watsons Triple Integrals}
\begin{itemize}
\item \url{http://mathworld.wolfram.com/WatsonsTripleIntegrals.html}
\item \url{http://www.inp.nsk.su/~silagadz/Watson_Integral.pdf}
\end{itemize}

\subsubsection{Generalized Calculus}
\begin{itemize}
\item \url{https://en.wikipedia.org/wiki/Product_integral}
\item \url{http://math2.org/math/paper/preface.htm}
\item \url{http://www.gauge-institute.org/calculus/PowerMeansCalculus.pdf}
\end{itemize}

\subsubsection{Finite calculus}
\begin{itemize}
\item \url{https://www.cs.purdue.edu/homes/dgleich/publications/Gleich\%202005\%20-\%20finite\%20calculus.pdf}
\item \url{https://en.wikipedia.org/wiki/Concrete_Mathematics}
\end{itemize}

\subsubsection{Iterative roots and fractional iteration}
\begin{itemize}
\item \url{http://reglos.de/lars/ffx.html}
\item \url{https://mathoverflow.net/questions/17605/how-to-solve-ffx-cosx}
\end{itemize}

\subsection{Geometry}
\begin{itemize}
\item \href{https://www.friedrich-verlag.de/fileadmin/redaktion/sekundarstufe/Mathematik/Der_Mathematikunterricht/Leseproben/Der_Mathematikunterricht_3_13_Leseprobe_2.pdf}{Cutting a cube along the diagonal}
\item \url{https://en.wikipedia.org/wiki/Visual_calculus}
\end{itemize}

\subsection{Weird constants and functions}

\subsubsection{Euler-Mascheroni constant}
\begin{itemize}
\item \url{https://en.wikipedia.org/wiki/Euler\%E2\%80\%93Mascheroni_constant#Generalizations}
\end{itemize}

\subsubsection{Universal Parabolic constant}
\begin{itemize}
\item \url{https://en.wikipedia.org/wiki/Universal_parabolic_constant}
\item \url{http://mathworld.wolfram.com/UniversalParabolicConstant.html}
\item \url{https://mathoverflow.net/questions/37871/is-it-a-coincidence-that-the-universal-parabolic-constant-shows-up-in-the-soluti}
\end{itemize}

\subsubsection{Apery's constant}
\begin{itemize}
\item \url{https://en.wikipedia.org/wiki/Ap%C3%A9ry%27s_constant}
\item \url{https://math.stackexchange.com/questions/12815/riemann-zeta-function-at-odd-positive-integers/12819#12819}
\end{itemize}

\subsubsection{Gauss's constant}
\begin{itemize}
\item \url{https://en.wikipedia.org/wiki/Gauss\%27s_constant}
\item \url{https://en.wikipedia.org/wiki/Lemniscatic_elliptic_function}
\item \url{https://en.wikipedia.org/wiki/Particular_values_of_the_Gamma_function}
\end{itemize}

\subsubsection{Riemann Zeta function}
\begin{itemize}
\item \url{https://math.stackexchange.com/questions/1792755/connection-between-the-area-of-a-n-sphere-and-the-riemann-zeta-function}
\item \url{https://suryatejag.wordpress.com/2011/11/24/riemann-functional-equation-and-hamburgers-theorem}
\end{itemize}


\subsection{Probability}
\begin{itemize}
\item \url{https://en.wikipedia.org/wiki/Secretary_problem}
\item \url{https://en.wikipedia.org/wiki/Kelly_criterion}
\end{itemize}

\subsubsection{Sample size}
\begin{itemize}
\item \url{https://stats.stackexchange.com/questions/192199/derivation-of-formula-for-sample-size-of-finite-population/192601#192601}
\item \url{https://math.stackexchange.com/questions/926478/how-does-accuracy-of-a-survey-depend-on-sample-size-and-population-size/1357604#1357604}
\item \url{https://onlinecourses.science.psu.edu/stat414/node/264}
\item \url{http://www.surveysystem.com/sscalc.htm}
\item \url{http://research-advisors.com/tools/SampleSize.htm}
\end{itemize}


