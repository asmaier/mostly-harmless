\chapter{Mathematics}

\section{Matrices}

In the following we want to restrict our discussion to matrices with
real entries.

\subsection{Square matrices}
Every square matrix can be split into a symmetric and an antisymmetric
(skew-symmetric) part 
\begin{align}
A=\underbrace{\frac{1}{2}\lra{A + A^T}}_{\text{symmetric}}
+\underbrace{\frac{1}{2}\lra{A - A^T}}_{\text{antisymmetric}}
\end{align}


\subsection{Symmetric matrices}

A symmetric matrix is a square matrix, that is equal to it's transpose.
\begin{align}
A = A^T
\end{align} 
The sum of two symmetric matrices is again a symmetric matrix, but
the product of two matrices is in general not symmetric. The product of
two symmetric matrices $A$ and $B$ is symmetric only, if the two
matrices commute: 
\begin{align}
AB = (AB)^T \ \text{if}\ AB = BA
\end{align} 
Symmetric matrices with real entries have only real eigenvalues. That
is why in principle, every symmetric matrix is equivalent to a diagonal
matrix with its eigenvalues being the entries on the diagonal.

\subsection{Applications of Matrices}
\subsubsection{Solving unsolvable equations}

We start with a simple equation: 
\begin{align}
A^2 * 1 = 1 
\end{align} 
What is the transformation A, when applied twice, which turns 1 into
1 ? The answer is simple: $A=1$ or $A=-1$. But what is the
transformation $B$, when applied twice, which turns 1 into -1 ? 
\begin{align}
B^2 * 1 = -1 
\end{align} 
The answer it turns out, is not so simple. Based on the common rules
of multiplication for real numbers, there seems to be no way to solve
this equation for $B$.

However there is a way out of the dilemma. The reader might have
noticed, that we didn't call $A$ or $B$ a number or a variable, but
a transformation. We could have also called it also an operator. This
should be a hint, that maybe simple numbers are not enough to solve such
an equation.

So let's think a bit. Can this equation maybe interpreted in a
geometrical way? Let's imagine the line of numbers. What we want is a
way, to move or transform the point 1 to the point -1, but doing it with
two steps (One step would be simple, this could be reflection at the
zero point). If we draw the line on a sheet of paper and look from far
away the solution might be become more obvious: The line is embedded on
the two dimensional surface of the paper! So we might interpret our
source point 1 in fact as a vector (1,0) and our target point -1 as
vector (-1,0). Maybe this helps. Let's rewrite our equation in two
dimensions 
\begin{align}
B^2 * \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} -1 \\ 0 \end{pmatrix} 
\end{align} 
So what we want is a transformation $B$, which when used twice on
the vector (1,0) will turn the vector into the vector (-1,0). Vector
(-1,0) is pointing into the opposite direction of vector (1,0), so it is
basically rotated by 180 degrees. So we are searching for a
transformation which rotates a vector in two steps by 180 degrees. Now
it should be obvious that the solution for $B$ must be a
transformation, which rotates a vector by 90 degrees, in short
$B = R(90^{\circ})$ (or a rotation in the other direction by -90
degrees).

So we found a solution by geometrical intuition, let's try to make it
precise. From linear algebra we know, that a transformation which turns
a 2D vector into another 2D vector must be a 2x2 matrix. So let's
rewrite the equation as a matrix equation 
\begin{align}
\begin{pmatrix} a & b \\ c & d \end{pmatrix} * \begin{pmatrix} a & b \\ c & d \end{pmatrix} * \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} -1 \\ 0 \end{pmatrix} 
\end{align} 
If we compute the matrix product on the left hand side we get 
\begin{align}
\begin{pmatrix} a^2 + bc & b (a+d) \\ c(a+d) & d^2 + bc \end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} -1 \\ 0 \end{pmatrix} 
\end{align} 
Further simplifying we get two equations 
\begin{align}
a^2+bc = -1 && ac + cd = 0
\end{align} 
To solve this equation one might attempt to set $c=0$. But in this
case we would end up where we started because the equation left would be
$a^2=-1$. So we have to assume $c \neq 0$ to find a sensible
solution. So we get 
\begin{align}
c = -\frac{a^2+1}{b} && a = -d
\end{align} 
From these equation we see that we also have to assume $b \neq 0$.
But without loss of generality we can set $a=0$,$d=0$ and are left
with 
\begin{align}
bc = -1
\end{align} 
If we restrict ourself to integer numbers, we finally have two
solutions for our transformation 
\begin{align}
B = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}  && B^* = \begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix}
\end{align} 
So what seemed impossible to solve in 1D with simple numbers turned
out to have quite simple solutions in 2D in the form of 2x2 matrices.

\subsubsection{Solving polynomial equations}
A polynomial equation of order $n$ is an equation that look like 
\begin{align}
a_nx^n + a_{n-1}x^{n-1} + \cdots + a_1 x + a_0 = 0
\end{align} 
Solving these kind of equations has been a hobby of mathematicans
since the invention of math. But whereas quadratic equations ($n=2$)
could be solved since ancient times, finding a general solution for
cubic ($n=3$) and quartic ($n=4$) equations turned out to be much
harder. Rafael Bombelli made a crucial step in 1572 when - in a
desparate move - he invented complex numbers to solve cubic equations,
which had been unsolvable up to that time. Had he known matrices and
linear algebra, the invention of ``complex numbers'' would have been
unnecessary.

This is so, because there is a deep connection between polynominal
equations and matrices. Actually it turns out that \textbf{any
polynomial with degree $n$ is the characteristic polynomial of some
\href{https://en.wikipedia.org/wiki/Companion_matrix}{companion matrix}
of order $n$}. So the problem of solving a polynominal equation is
equivalent to solving the characteristic equation of the companion
matrix. But solving the characteristic equation of a matrix means
computing the eigenvalues of that matrix. And computing eigenvalues has
a simple geometric meaning: They give the factor by which an eigenvector
(a vector which direction is left unchanged by the matrix
transformation) is stretched by the matrix transformation. The
eigenvalues are the scale factors of the linear transformation
represented by the matrix. This means that solving a polynomial equation
is equivalent to computing the scale factors of a corresponding linear
transformation.

Knowning this we can nowadays understand, why some polynominal equations
have no solutions. These are the equations, which correspond to
matrices, which don't have eigenvalues, meaning the transformation
doesn't leave any vector unchanged. From linear algebra we know, that
these transformations describe rotations. And this is the connection to
the 2d rotation matrix we found in the previous chapter. The polynominal
equation 
\begin{align}
x^2 = -1
\end{align} 
has no solution, because it corresponds to a matrix describing a 90
degree rotation.

On the other side the
\href{https://en.wikipedia.org/wiki/Fundamental_theorem_of_algebra}{fundamental
theorem of algebra} is easy to understand, when thinking of polynomials
as being represented by matrices. The number of solutions to a
polynomial of degree $n$ is the same as the number of eigenvalues of
the corresponding companion matrix.

\section{Complex Numbers}
Complex numbers are not numbers. They cannot be ordered according to their size. This basic insight makes clear that trying to work with complex numbers like with usual “real numbers” must fail (e.g. division doesn’t work) and in general is also the reason for the big confusion around them.

But complex numbers are also no vectors (in the geometrical sense). The multiplication rule for complex numbers is completely different from the usual scalar product of geometrical vectors. Multiplying two complex numbers yields another complex number, whereas the usual scalar multiplication of geometrical vectors yields a scalar. So although a complex number can be represented as a set of two numbers, this set of two numbers should not(!) be visualized as geometrical vector.

Instead the modern view is that complex numbers are 2D matrices. They represent the group of (antisymmetric) 2D rotation matrices (\url{https://en.wikipedia.org/wiki/Complex_number#Matrix_representation_of_complex_numbers}). All the features of complex numbers follow naturally from this representation, e.g. multiplying two matrices yields another matrix. Unfortunately most textbooks don’t even mention the matrix representation of complex numbers, although this really makes clear what complex “numbers” are, how they can be extended (e.g. quaternions are 3D rotation matrices) and how they fit into the bigger picture which is \url{https://en.wikipedia.org/wiki/Group_theory}.

\section{Linear algebra}

\begin{itemize}
\item \url{https://physics.stackexchange.com/questions/35562/is-a-1d-vector-also-a-scalar}
\item \url{https://math.stackexchange.com/questions/219434/is-a-one-by-one-matrix-just-a-number-scalar}
\item \url{https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Eigenvalues_of_geometric_transformations}
\end{itemize}
\subsection{Functions of matrices}
\begin{itemize}
\item \url{https://en.wikipedia.org/wiki/Matrix_function}
\item \url{https://en.wikipedia.org/wiki/Matrix_exponential}
\item \url{https://en.wikipedia.org/wiki/Logarithm_of_a_matrix}
\item \url{https://math.stackexchange.com/questions/1149598/how-to-solve-a-non-linear-matrix-equation-over-integer-numbers}
\end{itemize}
\section{Transformations and groups}

\begin{itemize}
\item Prove of eulers formula as a solution to 2d wave equation: \url{http://math.stackexchange.com/a/3512/27609}
\item \url{https://en.wikipedia.org/wiki/Linear_canonical_transformation}
\item \url{https://en.wikipedia.org/wiki/Hartley_transform}
\item \url{https://en.wikipedia.org/wiki/Split-complex_number}
\item \url{https://en.wikipedia.org/wiki/Dual_number} (\url{https://math.stackexchange.com/questions/1120720/are-dual-numbers-a-special-case-of-grassmann-number})
\item \url{https://en.wikipedia.org/wiki/Grassmann_number} \href{https://math.stackexchange.com/questions/1108045/relationship-between-levi-civita-symbol-and-grassmann-numbers}{Grassmann vectors}
\item \url{https://en.wikipedia.org/wiki/Quaternion} (\url{https://math.stackexchange.com/questions/147166/does-my-definition-of-double-complex-noncommutative-numbers-make-any-sense})
\item \url{https://math.stackexchange.com/questions/2083950/relationship-between-levi-civita-symbol-and-complex-quaternionic-numbers}
\end{itemize}
\section{Series}

\begin{itemize}
\item \url{http://blog.wolfram.com/2014/08/06/the-abcd-of-divergent-series}
\begin{itemize}
\item \url{http://physicsbuzz.physicscentral.com/2014/01/redux-does-1234-112-absolutely-not.html}
\item \url{https://www.quora.com/Whats-the-intuition-behind-the-equation-1+2+3+-cdots-tfrac-1-12}
\end{itemize}
\end{itemize}

\section{Calculus}

\subsection{Euler-MacLaurin}
\begin{itemize}
\item \url{https://people.csail.mit.edu/kuat/courses/euler-maclaurin.pdf}
\item \url{http://www.hep.caltech.edu/~phys199/lectures/lect5_6_ems.pdf}
\item \url{https://terrytao.wordpress.com/2010/04/10/the-euler-maclaurin-formula-bernoulli-numbers-the-zeta-function-and-real-variable-analytic-continuation}
\end{itemize}

\subsection{Watsons Triple Integrals}
\begin{itemize}
\item \url{http://mathworld.wolfram.com/WatsonsTripleIntegrals.html}
\item \url{http://www.inp.nsk.su/~silagadz/Watson_Integral.pdf}
\end{itemize}

\subsection{Generalized Calculus}
\begin{itemize}
\item \url{https://en.wikipedia.org/wiki/Product_integral}
\item \url{http://math2.org/math/paper/preface.htm}
\item \url{http://www.gauge-institute.org/calculus/PowerMeansCalculus.pdf}
\end{itemize}

\subsection{Finite calculus}
\begin{itemize}
\item \url{https://www.cs.purdue.edu/homes/dgleich/publications/Gleich\%202005\%20-\%20finite\%20calculus.pdf}
\item \url{https://en.wikipedia.org/wiki/Concrete_Mathematics}
\end{itemize}

\subsection{Iterative roots and fractional iteration}
\begin{itemize}
\item \url{http://reglos.de/lars/ffx.html}
\item \url{https://mathoverflow.net/questions/17605/how-to-solve-ffx-cosx}
\end{itemize}

\section{Geometry}
\begin{itemize}
\item \href{https://www.friedrich-verlag.de/fileadmin/redaktion/sekundarstufe/Mathematik/Der_Mathematikunterricht/Leseproben/Der_Mathematikunterricht_3_13_Leseprobe_2.pdf}{Cutting a cube along the diagonal}
\item \url{https://en.wikipedia.org/wiki/Visual_calculus}
\end{itemize}

\section{Weird constants and functions}

\subsection{Euler-Mascheroni constant}
\begin{itemize}
\item \url{https://en.wikipedia.org/wiki/Euler\%E2\%80\%93Mascheroni_constant#Generalizations}
\end{itemize}

\subsection{Universal Parabolic constant}
\begin{itemize}
\item \url{https://en.wikipedia.org/wiki/Universal_parabolic_constant}
\item \url{http://mathworld.wolfram.com/UniversalParabolicConstant.html}
\item \url{https://mathoverflow.net/questions/37871/is-it-a-coincidence-that-the-universal-parabolic-constant-shows-up-in-the-soluti}
\end{itemize}

\subsection{Apery's constant}
\begin{itemize}
\item \url{https://en.wikipedia.org/wiki/Ap%C3%A9ry%27s_constant}
\item \url{https://math.stackexchange.com/questions/12815/riemann-zeta-function-at-odd-positive-integers/12819#12819}
\end{itemize}

\subsection{Gauss's constant}
\begin{itemize}
\item \url{https://en.wikipedia.org/wiki/Gauss\%27s_constant}
\item \url{https://en.wikipedia.org/wiki/Lemniscatic_elliptic_function}
\item \url{https://en.wikipedia.org/wiki/Particular_values_of_the_Gamma_function}
\end{itemize}

\subsection{Riemann Zeta function}
\begin{itemize}
\item \url{https://math.stackexchange.com/questions/1792755/connection-between-the-area-of-a-n-sphere-and-the-riemann-zeta-function}
\item \url{https://suryatejag.wordpress.com/2011/11/24/riemann-functional-equation-and-hamburgers-theorem}
\end{itemize}


\section{Probability}
\begin{itemize}
\item \url{https://en.wikipedia.org/wiki/Secretary_problem}
\item \url{https://en.wikipedia.org/wiki/Kelly_criterion}
\end{itemize}

\subsection{Sample size}
\begin{itemize}
\item \url{https://stats.stackexchange.com/questions/192199/derivation-of-formula-for-sample-size-of-finite-population/192601#192601}
\item \url{https://math.stackexchange.com/questions/926478/how-does-accuracy-of-a-survey-depend-on-sample-size-and-population-size/1357604#1357604}
\item \url{https://onlinecourses.science.psu.edu/stat414/node/264}
\item \url{http://www.surveysystem.com/sscalc.htm}
\item \url{http://research-advisors.com/tools/SampleSize.htm}
\end{itemize}


